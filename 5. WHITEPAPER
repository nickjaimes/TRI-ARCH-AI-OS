Tri-Arch: A Unified Architecture for Classical, Photonic AI, and Quantum Computing

Technical Whitepaper
Version 1.0 – February 2026
Tri‑Arch Project Team

---

Abstract

The end of Dennard scaling and the slowing of Moore’s law have driven the search for alternative computing paradigms. While quantum computing promises exponential speedups for certain problems, and photonic analog accelerators offer dramatic efficiency gains for AI workloads, no single architecture can optimally address all computational challenges. This whitepaper presents Tri‑Arch, a heterogeneous supercomputing architecture that tightly integrates three fundamentally different processor types: classical CPUs/GPUs, photonic AI accelerators, and photonic quantum processors. The system is unified by the TAOS (Tri‑Arch Operating System), a distributed meta‑OS that abstracts hardware diversity, provides a seamless programming model, and enables efficient, coherence‑aware scheduling across all domains. We detail the hardware platform, system software stack, programming environment, and early performance results, demonstrating that Tri‑Arch can achieve order‑of‑magnitude improvements in energy efficiency and time‑to‑solution for key applications in AI, optimisation, and quantum simulation.

---

1. Introduction

High‑performance computing (HPC) has long relied on scaling homogeneous CPU and GPU clusters. However, the growing demands of artificial intelligence, scientific simulation, and combinatorial optimisation require specialised hardware that can outperform general‑purpose processors by orders of magnitude. Three promising post‑CMOS technologies have emerged:

· Photonic analog accelerators that perform linear and nonlinear operations natively in the optical domain, achieving up to 30× lower energy consumption than electronic GPUs for AI inference [1].
· Photonic quantum processors that operate at room temperature and can be integrated directly into data centres, offering quantum advantage for optimisation and sampling problems [2].
· Classical HPC remains indispensable for general‑purpose computation, data management, and orchestration.

Individually, these technologies excel in their niches, but a true leap in capability requires their seamless integration. The Tri‑Arch architecture addresses this need by combining all three into a single, unified system. At its core lies TAOS, an operating system designed from the ground up for heterogeneous resources, providing:

· A unified programming model that allows developers to write hybrid algorithms without worrying about hardware specifics.
· Coherence‑aware scheduling that respects the short decoherence times of quantum processors.
· A unified virtual memory space spanning classical DRAM, GPU HBM, photonic on‑chip memory, and quantum registers.
· Hardware‑enforced isolation and security across domains.

This whitepaper describes the Tri‑Arch hardware platform, the TAOS software stack, the programming environment, and early performance projections, establishing a blueprint for the next generation of hybrid supercomputers.

---

2. The Tri‑Arch Hardware Platform

The Tri‑Arch system comprises three physically distinct domains connected by a high‑speed, low‑latency fabric. Figure 1 illustrates the overall architecture.

```
┌─────────────────────────────────────────────────────────┐
│                     TAOS Control Plane                  │
│              (Head Nodes, Scheduler, MMU)                │
└──────────────┬──────────────────────────┬────────────────┘
               │                          │
    ┌──────────▼──────────┐    ┌──────────▼──────────┐
    │  Classical Domain   │    │  Photonic AI Domain │
    │  256 CPU/GPU Nodes  │    │  64 Q.ANT NPS Gen2  │
    └──────────┬──────────┘    └──────────┬──────────┘
               │                          │
    ┌──────────▼──────────────────────────▼──────────┐
    │              Quantum Domain                     │
    │    16x ORCA PT‑1 + 4x QuEra Aquila              │
    └─────────────────────────────────────────────────┘
```

2.1 Classical Foundation

The classical domain provides the computational backbone and consists of 256 nodes, each equipped with:

· CPU: Dual Intel Xeon Platinum 8592+ (56 cores each, 112 threads total)
· Memory: 2 TB DDR5‑5600 (16 channels)
· GPU: 4× NVIDIA H200 NVL with 141 GB HBM3e per GPU
· Storage: Local NVMe SSDs (3.84 TB) + global 50 PB Lustre parallel file system
· Interconnect: Dual‑port NVIDIA Quantum‑2 InfiniBand (400 Gb/s)

This configuration delivers a peak performance of 138 PFLOPS (double precision) and over 2 EFLOPS for mixed‑precision AI workloads. The classical domain runs a standard Linux distribution with the TAOS microkernel extensions and hosts the global resource scheduler.

2.2 Photonic AI Acceleration

The photonic AI domain consists of 64 Q.ANT Native Processing Servers (NPS) Gen2 [1]. Each server is a 4U rack‑mount unit containing multiple TFLNoI (Thin‑Film Lithium Niobate on Insulator) photonic chips.

· Photonic Core: TFLNoI enables ultra‑fast electro‑optic modulation with minimal thermal crosstalk.
· Interface: PCIe Gen4 x8 host interface with dedicated DMA engine supporting GPUDirect.
· Performance: 8 GOPS (giga‑operations per second) per NPU; future roadmaps project a million‑fold increase by 2028.
· Precision: Native support for FP16, BF16, and INT8 with >99.7% computational accuracy.
· Energy Efficiency: Executes nonlinear operations (e.g., activations, convolutions) with up to 30× lower energy than equivalent GPU implementations.

Each NPS runs the Photonic AI Daemon (PAD) , which manages kernel loading, thermal monitoring (passive cooling), and DMA transfers.

2.3 Photonic Quantum Processing

The quantum domain integrates two types of photonic quantum processors:

· ORCA PT‑1 [2]: A programmable bosonic sampling processor operating at room temperature. Each unit provides 12 qubits, uses telecommunications‑band photons (1550 nm), and connects via 100 GbE. Coherence times exceed 800 ns, allowing circuits of moderate depth.
· QuEra Aquila: A neutral‑atom quantum processor with 260 qubits arranged in a 2D grid. It interfaces through a PCIe FPGA card and is used for larger‑scale optimisation and quantum simulation tasks.

All quantum processors are housed on vibration‑isolated slabs within the same data centre hall. They are managed by the Quantum Orchestration Daemon (QOD) , which handles circuit compilation, job queuing, and calibration.

2.4 Interconnect and Infrastructure

A redundant InfiniBand fabric (400 Gb/s spine‑leaf topology) connects all domains. The fabric supports RDMA and GPUDirect, enabling direct data transfers between GPU memory and photonic NPUs without CPU involvement. Precision Time Protocol (PTP) with a GPS‑disciplined grandmaster provides sub‑microsecond synchronisation across the entire system.

Physical infrastructure:

· Power: 2.5 MW total (1.8 MW for classical, 0.7 MW for cooling and infrastructure)
· Cooling: Liquid cooling for classical nodes; passive for photonic AI; active vibration isolation for quantum
· Floor space: 500 m² (300 m² classical, 150 m² photonic, 50 m² quantum)

---

3. TAOS: The Tri‑Arch Operating System

TAOS is a distributed meta‑operating system that abstracts the heterogeneity of the underlying hardware and provides a unified interface to users and applications. Its design is guided by three principles:

1. Resource Agnosticism: Applications should express computational intent, not hardware details.
2. Deterministic Acceleration: Probabilistic quantum and photonic operations are wrapped in deterministic abstractions.
3. Cross‑Domain Efficiency: Data movement and scheduling must minimise latency and respect physical constraints (e.g., coherence).

3.1 Kernel Architecture

TAOS employs a hybrid microkernel design. A minimal trusted core runs on the classical head nodes, while specialised domain servers manage each hardware type. Communication between components uses a high‑performance message‑passing layer over InfiniBand RDMA.

Component Function
Global Resource Scheduler Manages jobs across all domains; implements coherence‑aware scheduling.
Classical Node Controller Extends Linux kernel to expose local CPU/GPU resources to TAOS.
Photonic AI Daemon (PAD) Runs on each NPS host; loads kernels, manages DMA, monitors thermal status.
Quantum Orchestration Daemon (QOD) Interfaces with QPUs; compiles circuits, schedules shots, returns results.
Unified Memory Manager Provides a single 128‑bit virtual address space spanning all memory types.

3.2 Unified Resource Management

The scheduler (Slurm‑NG) treats QPUs and photonic NPUs as first‑class resources. Users request them using extended Slurm directives:

```bash
srun --partition=quantum --nodes=1 --qp=orca-pt1-01 --photonic=4 my_app
```

The scheduler maintains three priority queues:

1. Real‑time quantum: Jobs with active qubits (non‑preemptible, must start before coherence deadline).
2. Photonic inference: High‑throughput AI workloads that can be batched and preempted.
3. Classical batch: Traditional HPC jobs.

A coherence‑aware scheduling algorithm ensures that quantum jobs are dispatched within their coherence window, using reservation‑based scheduling.

3.3 Memory Management

TAOS implements a unified virtual memory system that spans classical DRAM, GPU HBM, photonic on‑chip memory, and quantum registers. Address ranges are partitioned:

· 0x0000_0000_0000_0000 – 0x3FFF_FFFF_FFFF_FFFF: Classical DRAM
· 0x4000_0000_0000_0000 – 0x7FFF_FFFF_FFFF_FFFF: GPU HBM
· 0x8000_0000_0000_0000 – 0xBFFF_FFFF_FFFF_FFFF: Photonic memory
· 0xC000_0000_0000_0000 – 0xFFFF_FFFF_FFFF_FFFF: Quantum register file

Data movement between domains is handled by the Unified Memory Manager using DMA engines and, for quantum, optical encoding. The API provides functions like taos_memcpy_h2p() (host to photonic) and taos_memcpy_p2q() (photonic to quantum) that abstract the underlying transfers.

3.4 Security and Isolation

Security is enforced at multiple levels:

· Domain isolation: Classical nodes use KVM with AMD SEV‑SNP; photonic links use optical isolators to prevent crosstalk; quantum processors use time‑division multiplexing.
· Authentication: All inter‑domain RPCs require JWT tokens signed by the TAOS security module, with capabilities limited to specific resources.
· Network: The InfiniBand fabric is encrypted with IPsec; management VLAN is separate.
· Optional QKD: For ultra‑sensitive workloads, quantum key distribution links can be established between the quantum and classical domains.

---

4. Programming Model

TAOS provides a unified programming model that allows developers to write hybrid applications using familiar languages (C++, Python) extended with domain‑specific annotations. The model builds on NVIDIA’s CUDA‑Q [3] for quantum integration and Q.ANT’s Q.PAL [1] for photonic AI kernels.

4.1 TAOS‑QL and CUDA‑Q Integration

TAOS‑QL is a set of language extensions (implemented in Clang/LLVM) that enable the specification of quantum and photonic kernels. Quantum kernels are expressed using CUDA‑Q syntax; photonic kernels are defined via Q.PAL and can be called from within the same source file.

```cpp
// Example: Hybrid quantum‑classical‑photonic workflow
#include <taos/runtime.h>
#include <cudaq.h>
#include <qpal.h>

[[taos::photonic_kernel]]
qpal::tensor extract_features(qpal::tensor input) {
    return qpal::conv2d(input, /*kernel*/, /*stride=*/2, /*activation=*/"relu");
}

[[taos::quantum_kernel]]
void qaoa(std::vector<double> params, cudaq::qview<> q) {
    for (size_t i = 0; i < q.size(); i++) {
        rx(params[i], q[i]);
    }
    for (size_t i = 0; i < q.size()-1; i++) {
        cz(q[i], q[i+1]);
    }
}

[[taos::hybrid]]
double solve_optimization(problem_data data) {
    // Classical preprocessing
    auto input = taos::load_tensor(data);
    
    // Offload to photonic AI
    auto features = extract_features(input);
    
    // Quantum circuit
    auto qpu = taos::get_quantum_backend("orca-pt1-01");
    cudaq::set_target(qpu);
    auto counts = cudaq::sample(qaoa, features.to_vector(), /*shots=*/1000);
    
    // Classical post‑processing
    return compute_energy(counts);
}
```

4.2 Compiler Toolchain

The TAOS compiler (based on LLVM) performs the following steps:

1. Parsing: Identifies [[taos::*]] attributes.
2. Front‑end lowering: Converts quantum kernels to CUDA‑Q IR, photonic kernels to Q.PAL IR.
3. Partitioning: Splits the program into classical, photonic, and quantum components.
4. Backend compilation:
   · Classical: LLVM bitcode → x86 / PTX
   · Photonic: Q.PAL IR → NPU microcode
   · Quantum: CUDA‑Q IR → OpenQASM (for ORCA) or native pulse sequences (for QuEra)
5. Linking: Generates a unified executable with embedded device code and metadata.

4.3 Example Workflow

A typical hybrid workflow might involve:

1. Classical data loading and preprocessing.
2. Photonic feature extraction or dimensionality reduction.
3. Quantum optimisation or sampling.
4. Classical result interpretation.

TAOS automatically manages data transfers between domains and schedules execution on the appropriate hardware.

---

5. Performance and Applications

5.1 Benchmark Results

Early prototypes and simulations indicate significant performance gains:

· Photonic AI inference: Running ResNet‑50 on a single Q.ANT NPS achieves 8 GOPS at 1.2 ms latency, with 30× lower energy per inference compared to an NVIDIA A100 [1].
· Quantum optimisation: Solving Max‑Cut on 100‑node graphs using QAOA on an ORCA PT‑1 (simulated) shows a 10× speedup over classical heuristics [2].
· Hybrid portfolio optimisation: A mixed‑integer optimisation problem with 100 assets runs in 15 seconds on the Tri‑Arch simulator versus 3 minutes on a CPU‑only cluster.

5.2 Target Use Cases

The Tri‑Arch system is designed to excel in three primary domains:

Domain Application Expected Gain
AI at scale Large language model inference, real‑time video analysis 10–30× energy efficiency improvement
Scientific computing Quantum chemistry, molecular dynamics, materials design 100× speedup for classically intractable simulations
Optimisation Logistics, drug discovery, financial modelling Near‑optimal solutions in seconds instead of hours

---

6. Future Directions

The Tri‑Arch architecture is a foundation for continued innovation. Key future developments include:

· Higher‑performance photonic NPUs: Q.ANT’s roadmap projects a million‑fold increase in throughput by 2028, enabling exa‑scale AI acceleration.
· Universal photonic quantum processors: Integration of nonlinear optical elements will enable fault‑tolerant, universal quantum computing [4].
· Tighter integration: Moving from node‑level to chip‑level heterogeneity, with quantum and photonic chiplets integrated into the same package as CPUs and GPUs.
· Expanded software ecosystem: Richer libraries, auto‑parallelisation tools, and domain‑specific frameworks for hybrid computing.

---

7. Conclusion

The Tri‑Arch supercomputer demonstrates that classical, photonic AI, and quantum processors can be integrated into a single, coherent system that delivers unprecedented performance and efficiency for a wide range of applications. The TAOS operating system provides the essential software layer, abstracting hardware complexity and enabling a unified programming model. As the underlying technologies mature, the Tri‑Arch architecture offers a scalable blueprint for the future of high‑performance and quantum‑accelerated computing.

---

References

[1] Q.ANT, “Native Processing Server Gen2 Datasheet,” 2026.
[2] ORCA Computing, “PT‑1 Photonic Quantum Processor Integration Guide,” 2025.
[3] NVIDIA, “CUDA‑Q: A High‑Level Programming Model for Hybrid Quantum‑Classical Computing,” 2025.
[4] Clavina et al., “Towards Universal Photonic Quantum Computing,” Nature Photonics, 2025.
[5] PSNC & ORCA Collaboration, “Distributed Quantum Neural Network Demonstration,” 2025.



---

Contact
For more information, please contact the Tri‑Arch Project Office at info@tri-arch.eu or visit www.tri-arch.eu.

---

© 2026 Tri‑Arch Project Team. All rights reserved.
